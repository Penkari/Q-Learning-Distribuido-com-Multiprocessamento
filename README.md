# â„ï¸ Q-Learning DistribuÃ­do com Multiprocessamento no FrozenLake

Este projeto explora o uso de **Q-learning com multiprocessing** para resolver o ambiente FrozenLake do Gymnasium, utilizando um mapa personalizado e ajustes de recompensa com wrappers.

Apesar de a polÃ­tica nÃ£o ter convergido totalmente ao final do treinamento, o projeto Ã© um estudo realista e tÃ©cnico sobre as limitaÃ§Ãµes e desafios do aprendizado por reforÃ§o distribuÃ­do.

---

## ğŸ¯ Objetivo

A proposta do projeto Ã© aplicar **Q-learning distribuÃ­do** em mÃºltiplos processos paralelos, para acelerar o aprendizado de um agente em um ambiente estocÃ¡stico com mÃºltiplos obstÃ¡culos (`H`) e um objetivo (`G`). O projeto tambÃ©m busca explorar variaÃ§Ãµes de recompensa por distÃ¢ncia ao objetivo.

---

## ğŸ§  Tecnologias e Conceitos Usados

- **Python 3.11+**
- `multiprocessing` para treino distribuÃ­do
- `gymnasium` (FrozenLake-v1 com `desc` personalizado)
- `RewardWrapper` e `ObservationWrapper`
- **Q-Table compartilhada entre processos**
- Decaimento de epsilon (Îµ) com valor mÃ­nimo
- ExportaÃ§Ã£o da Q-table em `.json`

---

## ğŸ§© Estrutura do Projeto

O agente foi treinado com os seguintes parÃ¢metros:

- `episÃ³dios = 200000` (distribuÃ­dos entre `num_processos = 8`)
- `epsilon` com decaimento exponencial e limite inferior (`min_epsilon`)
- `gamma = 0.96` (fator de desconto)
- `alpha = 0.81` (taxa de aprendizado)
- `is_slippery = False` (ambiente determinÃ­stico)

Wrappers adicionais foram usados para:

- Transformar observaÃ§Ãµes em coordenadas 2D (opcional)
- Modificar recompensas com base na distÃ¢ncia ao goal (recompensa suave)

---

## ğŸ”„ Funcionamento

Cada processo executa uma fraÃ§Ã£o dos episÃ³dios e atualiza **simultaneamente uma Q-table global compartilhada**, protegida por `Lock()` para evitar race conditions.

A lÃ³gica principal do agente:
1. Escolher aÃ§Ã£o via Îµ-greedy
2. Atualizar a Q-table com a equaÃ§Ã£o de Bellman
3. Repetir atÃ© atingir `goal`, `done` ou `truncation`

ApÃ³s o treinamento, o modelo Ã© avaliado em episÃ³dios sem exploraÃ§Ã£o (`epsilon = 0`), utilizando apenas as decisÃµes aprendidas (`argmax(Q(s, a))`).

---

## âš ï¸ Status do Projeto

Apesar da estrutura funcionar e os dados serem processados corretamente:

- **O agente nÃ£o conseguiu convergir consistentemente** para alcanÃ§ar o objetivo (`G`) no mapa personalizado.
- HipÃ³teses incluem:
  - Recompensas suaves demais
  - EspaÃ§o de estados subutilizado
  - LimitaÃ§Ãµes naturais do Q-learning tabular em mapas complexos

> O projeto foi pausado temporariamente e serÃ¡ retomado futuramente com mais recursos, ajustes de lÃ³gica e possÃ­vel transiÃ§Ã£o para mÃ©todos baseados em polÃ­ticas.

---

## ğŸ“ Arquivos

- `aprendizado.py`: cÃ³digo principal com treino, wrappers e avaliaÃ§Ã£o
- `q_table_exportada.json`: exportaÃ§Ã£o da Q-table gerada apÃ³s o treinamento
- `README.md`: documentaÃ§Ã£o do projeto

---

## ğŸš§ Futuras Melhorias

- Implementar renderizaÃ§Ã£o visual do caminho Ã³timo
- ReduÃ§Ã£o de penalidades excessivas
- AutomatizaÃ§Ã£o da geraÃ§Ã£o de mapas e recompensas
- AdiÃ§Ã£o de logs e grÃ¡ficos de desempenho

---

## âœï¸ Autor

Eduardo Oliveira  
Desenvolvedor de IA Independente  
[LinkedIn](https://www.linkedin.com/in/eduardo-oliveira-971097240/)

---

> *â€œNinguÃ©m sabe como se faz, atÃ© que se aprendeâ€*
